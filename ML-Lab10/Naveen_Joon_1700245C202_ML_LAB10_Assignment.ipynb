{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability =  0.21691\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "massDist = (0.2,0.4,0.1,0.1,0.1,0.1)\n",
    "def roll(massDist):\n",
    "    randRoll = random.random() # in [0,1]\n",
    "    sum = 0\n",
    "    result = 1\n",
    "    for mass in massDist:\n",
    "        sum += mass\n",
    "        if randRoll < sum:\n",
    "            return result\n",
    "        result+=1\n",
    "\n",
    "def dice(massDict):\n",
    "    step = 0;\n",
    "    numstep = 0;\n",
    "    for i in range(100000):\n",
    "        for i in range(250):\n",
    "            result = roll(massDict)\n",
    "            # print(\"You rolled\",result)\n",
    "        if result==1 or result==2: step=max(0,step-1)\n",
    "        elif result>=3 and result<=5: step=step+1\n",
    "        else: \n",
    "            result1 = roll(massDict)\n",
    "            step=step+result1\n",
    "        if step>60: numstep=numstep+1\n",
    "    return numstep\n",
    "\n",
    "numstep = dice(massDist)\n",
    "print(\"Probability = \",numstep/100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Data for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X0        X1        X2        X3         Y\n",
      "0 -0.381628  1.197031  0.929858 -1.554308  0.940842\n",
      "1  0.136436 -1.233528  0.410580  1.138531  0.965800\n",
      "2  0.018444  0.100027 -0.586007 -0.134705  1.095683\n",
      "3  0.402928  0.921435  0.290072 -1.476784  0.912180\n",
      "4 -0.155967 -0.020985  1.660527 -1.316407  0.822326\n",
      "          X0        X1        X2        X3         Y\n",
      "95  0.017886  0.755963  0.926295  1.140623  2.286016\n",
      "96  1.758766  0.412127  0.003410  0.259682  1.921530\n",
      "97 -0.695840 -0.472389 -0.103946 -0.152054  0.234136\n",
      "98  0.362451 -0.226238 -1.234896  0.523522  0.886967\n",
      "99  1.020785  2.351176  1.339573  0.139454  3.456115\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      "X0    100 non-null float64\n",
      "X1    100 non-null float64\n",
      "X2    100 non-null float64\n",
      "X3    100 non-null float64\n",
      "Y     100 non-null float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 4.0 KB\n",
      "None\n",
      "               X0          X1          X2          X3           Y\n",
      "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
      "mean     0.006177    0.017036   -0.091310    0.256152    1.114653\n",
      "std      0.906718    0.964912    0.930681    0.960137    0.786099\n",
      "min     -2.391797   -2.113601   -1.898774   -2.715246   -0.788723\n",
      "25%     -0.445610   -0.537697   -0.853984   -0.370086    0.631592\n",
      "50%      0.018165   -0.018553   -0.165377    0.284376    1.097755\n",
      "75%      0.484436    0.663396    0.537970    0.931879    1.638853\n",
      "max      2.180023    2.619097    2.009441    3.053817    3.456115\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "random.seed(1)\n",
    "n_features = 4\n",
    "X = []\n",
    "for i in range(n_features):\n",
    "    X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
    "    X.append(X_i)\n",
    "#print(X)\n",
    "eps = scipy.stats.norm.rvs(0, 0.25,100)\n",
    "y = 1 + (0.4 * X[0]) + eps + (0.5 * X[1]) + (0.3 * X[2]) + (0.4 * X[3])\n",
    "data_mlr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y }\n",
    "df = pd.DataFrame(data_mlr)\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "#df.to_csv('file1.csv') - from this statement we can generate it to csv format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Data for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X0        X1        X2        X3  Y\n",
      "0  0.504857 -0.055427  0.227699 -0.947237  1\n",
      "1 -1.586259  1.453189 -0.726366 -0.476809  1\n",
      "2  0.470961  0.591235 -0.314390  1.407829  1\n",
      "3  0.306974  0.166083  1.070944  0.532603  1\n",
      "4 -0.562862 -1.179548  1.953966  0.104510  1\n",
      "          X0        X1        X2        X3  Y\n",
      "95 -0.150128  1.564942  0.521429  0.698388  1\n",
      "96  0.575497 -1.176935  0.607625  1.193475  1\n",
      "97 -0.569024  0.641095  1.493383  0.354178  1\n",
      "98 -0.810205 -0.180606 -1.403905 -0.109654  1\n",
      "99 -1.012058 -0.362206 -0.375748  1.922488  1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      "X0    100 non-null float64\n",
      "X1    100 non-null float64\n",
      "X2    100 non-null float64\n",
      "X3    100 non-null float64\n",
      "Y     100 non-null int64\n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 4.0 KB\n",
      "None\n",
      "               X0          X1          X2          X3           Y\n",
      "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
      "mean    -0.192157    0.058505    0.024572    0.087124    0.870000\n",
      "std      0.870556    0.983164    0.965046    1.031105    0.337998\n",
      "min     -2.375061   -2.573722   -3.132826   -2.944210    0.000000\n",
      "25%     -0.828587   -0.633177   -0.491558   -0.599309    1.000000\n",
      "50%     -0.189187    0.060952   -0.137572    0.018042    1.000000\n",
      "75%      0.393047    0.745554    0.699558    0.725458    1.000000\n",
      "max      1.593962    2.374504    2.858560    2.547933    1.000000\n"
     ]
    }
   ],
   "source": [
    "n_features = 4\n",
    "X = []\n",
    "for i in range(n_features):\n",
    "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
    "  X.append(X_i)\n",
    "#print(X)\n",
    "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
    "#print(a1)\n",
    "y1 = []\n",
    "for i in a1:\n",
    "  if (i>=0.5):\n",
    "    y1.append(1)\n",
    "  else:\n",
    "    y1.append(0)\n",
    "#print(y1)\n",
    "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
    "df1 = pd.DataFrame(data_lr)\n",
    "print(df1.head())\n",
    "print(df1.tail())\n",
    "print(df1.info())\n",
    "print(df1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Data for K means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "from sklearn.datasets import make_blobs\n",
    "# generate 2d classification dataset\n",
    "X3, y3 = make_blobs(n_samples=100, centers=3, n_features=2)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X3[:,0], y=X3[:,1], label=y3))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12033042925103404 0.21254722703720827\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:,0].values\n",
    "# print(X)\n",
    "y = df.iloc[:,4].values\n",
    "b1 = 0\n",
    "b0 = 0\n",
    "l = 0.001\n",
    "epochs = 100\n",
    " \n",
    "n = float(len(X))\n",
    "for i in range(epochs):\n",
    "  y_p = b1*X + b0\n",
    "  loss = np.sum(y_p - y1)**2\n",
    "  d1 = (-2/n) * sum(X * (y - y_p))\n",
    "  d0 = (-2/n) * sum(y - y_p)\n",
    "  b1 = b1 - (l*d1)\n",
    "  b0 = b0 - (l*d0)\n",
    "\n",
    "print(b1,b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "0.3858429083509646\n",
      "0.38530949767165595\n",
      "0.384786352217898\n",
      "0.3842733507884137\n",
      "0.3837703720146266\n",
      "0.38327729428922175\n",
      "0.38279399570346495\n",
      "0.38232035399346626\n",
      "0.38185624649550337\n"
     ]
    }
   ],
   "source": [
    "X1 = df1.iloc[:,0:4].values\n",
    "y1 = df1.iloc[:,4].values\n",
    "\n",
    "def sigmoid(Z):\n",
    "  return 1 /(1+np.exp(-Z))\n",
    "\n",
    "def loss(y1,y_hat):\n",
    "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
    "\n",
    "W = np.zeros((4,1))\n",
    "b = np.zeros((1,1))\n",
    "\n",
    "m = len(y1)\n",
    "lr = 0.001\n",
    "for epoch in range(1000):\n",
    "  Z = np.matmul(X1,W)+b\n",
    "  A = sigmoid(Z)\n",
    "  logistic_loss = loss(y1,A)\n",
    "  dz = A - y1\n",
    "  dw = 1/m * np.matmul(X1.T,dz)\n",
    "  db = np.sum(dz)\n",
    "\n",
    "  W = W - lr*dw\n",
    "  b = b - lr*db\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    print(logistic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regreesion using L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11122970402666771 0.21270111626376426\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:,0].values\n",
    "#print(X)\n",
    "y = df.iloc[:,4].values\n",
    "b1 = 0\n",
    "b0 = 0\n",
    "l = 0.001\n",
    "epochs = 100\n",
    "lam = 0.1\n",
    " \n",
    "n = float(len(X))\n",
    "for i in range(epochs):\n",
    "  y_p = b1*X + b0\n",
    "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
    "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
    "  d0 = (-2/n) * sum(y - y_p)\n",
    "  b1 = b1 - (l*d1)\n",
    "  b0 = b0 - (l*d0)\n",
    "\n",
    "print(b1,b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regreesion using L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11974924597113544 0.21255386283551186\n"
     ]
    }
   ],
   "source": [
    "#print(X)\n",
    "y = df.iloc[:,4].values\n",
    "b1 = 0\n",
    "b0 = 0\n",
    "l = 0.001\n",
    "epochs = 100\n",
    "lam = 0.1\n",
    " \n",
    "n = float(len(X))\n",
    "for i in range(epochs):\n",
    "  y_p = b1*X + b0\n",
    "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
    "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
    "  d0 = (-2/n) * sum(y - y_p)\n",
    "  b1 = b1 - (l*d1)\n",
    "  b0 = b0 - (l*d0)\n",
    "\n",
    "print(b1,b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using L1 regualrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "-0.012126098265757224\n",
      "-0.4064238509057354\n",
      "-0.7965907889663033\n",
      "-1.1826694640689903\n",
      "-1.5647031911633091\n",
      "-1.9427359777929496\n",
      "-2.3168124549435705\n",
      "-2.6869778095293464\n",
      "-3.0532777185729048\n"
     ]
    }
   ],
   "source": [
    "X1 = df1.iloc[:,0:4].values\n",
    "y1 = df1.iloc[:,4].values\n",
    "lam = 0.1\n",
    "def sigmoid(Z):\n",
    "  return 1 /(1+np.exp(-Z))\n",
    "\n",
    "def loss(y1,y_hat):\n",
    "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
    "\n",
    "W = np.zeros((4,1))\n",
    "b = np.zeros((1,1))\n",
    "\n",
    "m = len(y1)\n",
    "lr = 0.001\n",
    "for epoch in range(1000):\n",
    "  Z = np.matmul(X1,W)+b\n",
    "  A = sigmoid(Z)\n",
    "  logistic_loss = loss(y1,A)\n",
    "  dz = A - y1\n",
    "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
    "  db = np.sum(dz)\n",
    "\n",
    "  W = W - lr*dw\n",
    "  b = b - lr*db\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    print(logistic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using L2 regualrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "0.3863840673405913\n",
      "0.38743223327809967\n",
      "0.38947015016341324\n",
      "0.3924392173930635\n",
      "0.39628322637877567\n",
      "0.400948284924234\n",
      "0.40638274378174866\n",
      "0.4125371252591498\n",
      "0.41936405375954267\n"
     ]
    }
   ],
   "source": [
    "X1 = df1.iloc[:,0:4].values\n",
    "y1 = df1.iloc[:,4].values\n",
    "lam = 0.1\n",
    "def sigmoid(Z):\n",
    "  return 1 /(1+np.exp(-Z))\n",
    "\n",
    "def loss(y1,y_hat):\n",
    "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
    "\n",
    "W = np.zeros((4,1))\n",
    "b = np.zeros((1,1))\n",
    "\n",
    "m = len(y1)\n",
    "lr = 0.001\n",
    "for epoch in range(1000):\n",
    "  Z = np.matmul(X1,W)+b\n",
    "  A = sigmoid(Z)\n",
    "  logistic_loss = loss(y1,A)\n",
    "  dz = A - y1\n",
    "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
    "  db = np.sum(dz)\n",
    "\n",
    "  W = W - lr*dw\n",
    "  b = b - lr*db\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    print(logistic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self,data):\n",
    "\n",
    "        self.centroids = {}\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            self.classifications = {}\n",
    "\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "\n",
    "            for featureset in data:\n",
    "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classifications[classification].append(featureset)\n",
    "\n",
    "            prev_centroids = dict(self.centroids)\n",
    "\n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
    "\n",
    "            optimized = True\n",
    "\n",
    "            for c in self.centroids:\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
    "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
    "                    optimized = False\n",
    "\n",
    "            if optimized:\n",
    "                break\n",
    "\n",
    "    def predict(self,data):\n",
    "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557.5985795711925\n",
      "12.951464772499016\n",
      "4.31199051407958\n"
     ]
    }
   ],
   "source": [
    "clf = K_Means()\n",
    "clf.fit(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(X3)):\n",
    "    predict_me = np.array(X3[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = clf.predict(predict_me)\n",
    "    if prediction == y3[i]:\n",
    "        correct += 1\n",
    "print(correct/len(X3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression from scratch using OOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegressionModel():\n",
    "\n",
    "    def __init__(self, dataset, learning_rate, num_iterations):\n",
    "        self.dataset = np.array(dataset)\n",
    "        self.b = 0  \n",
    "        self.m = 0  \n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.M = len(self.dataset)\n",
    "        self.total_error = 0\n",
    "\n",
    "    def apply_gradient_descent(self):\n",
    "        for i in range(self.num_iterations):\n",
    "            self.do_gradient_step()\n",
    "\n",
    "    def do_gradient_step(self):\n",
    "        b_summation = 0\n",
    "        m_summation = 0\n",
    "        for i in range(self.M):\n",
    "            x_value = self.dataset[i, 0]\n",
    "            y_value = self.dataset[i, 1]\n",
    "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
    "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
    "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
    "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
    "      \n",
    "    def compute_error(self):\n",
    "        for i in range(self.M):\n",
    "            x_value = self.dataset[i, 0]\n",
    "            y_value = self.dataset[i, 1]\n",
    "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
    "        return self.total_error\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
    "\n",
    "    def get_prediction_based_on(self, x):\n",
    "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
    "def main():\n",
    "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
    "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
    "    lr.apply_gradient_descent()\n",
    "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    for hour in hours:\n",
    "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
    "    print(lr)\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression from scratch using OOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.num_iters = num_iters\n",
    "    self.fit_intercept = fit_intercept\n",
    "    self.verbose = verbose\n",
    "  def __add_intercept(self, X):\n",
    "    intercept = np.ones((X.shape[0],1))\n",
    "    return np.concatenate((intercept,X),axis=1)\n",
    "  def __sigmoid(self,z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "  def __loss(self, h, y):\n",
    "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
    "  \n",
    "  def fit(self,X,y):\n",
    "    if self.fit_intercept:\n",
    "      X = self.__add_intercept(X)\n",
    "    self.theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    for i in range(self.num_iters):\n",
    "      z = np.dot(X,self.theta)\n",
    "      h = self.__sigmoid(z)\n",
    "      gradient = np.dot(X.T,(h-y))/y.size\n",
    "      \n",
    "      self.theta -= self.learning_rate * gradient\n",
    "      \n",
    "      z = np.dot(X,self.theta)\n",
    "      h = self.__sigmoid(z)\n",
    "      loss = self.__loss(h,y)\n",
    "      \n",
    "      if self.verbose == True and i % 1000 == 0:\n",
    "        print(f'Loss: {loss}\\t')\n",
    "  def predict_probability(self,X):\n",
    "    if self.fit_intercept:\n",
    "      X = self.__add_intercept(X)\n",
    "    return self.__sigmoid(np.dot(X,self.theta))\n",
    "  def predict(self,X):\n",
    "    return (self.predict_probability(X).round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means from scratch using OOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self,data):\n",
    "\n",
    "        self.centroids = {}\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            self.classifications = {}\n",
    "\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "\n",
    "            for featureset in X:\n",
    "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classifications[classification].append(featureset)\n",
    "\n",
    "            prev_centroids = dict(self.centroids)\n",
    "\n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
    "\n",
    "            optimized = True\n",
    "\n",
    "            for c in self.centroids:\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
    "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
    "                    optimized = False\n",
    "\n",
    "            if optimized:\n",
    "                break\n",
    "    def predict(self,data):\n",
    "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification\n",
    "        \n",
    "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
