{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyOfL9FKDoG5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 1:\n",
    "Here you must read an input file. Each line contains 785 numbers (comma delimited): the first\n",
    "values are between 0.0 and 1.0 correspond to the 784 pixel values (black and white images), and the\n",
    "last number denotes the class label: 0 corresponds to digit 0, 1 corresponds to digit 1, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlcq1HAs7meI"
   },
   "source": [
    "## Solution 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USiZZJaUFQcT"
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "  df = pd.read_csv(filepath)\n",
    "  df.head()\n",
    "  Y = df['5'].to_numpy()\n",
    "  del df['5']\n",
    "  X=df.to_numpy()\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVXIEJCvD0dJ"
   },
   "outputs": [],
   "source": [
    "X, y = load_data(\"mnist_train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    " \n",
    "Labels=pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2:Implement the backpropagation algorithm in a zero hidden layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zyoHKGD7tvJ"
   },
   "source": [
    "## Solution 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aYMsXGyDwqJ"
   },
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   \n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "\n",
    "\n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            x=self.softmax(z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            prev_term=self.delta_mll(y,self.y_pred)   \n",
    "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            x=self.softmax(z) \n",
    "        return np.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "TjgzI7Ss7dbD",
    "outputId": "fe2eff6f-534a-4c6a-ba87-f4a4e6f664c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0007346536093938932\n",
      "1 0.001333487169224761\n",
      "2 0.0013481239936291142\n",
      "3 0.00140891863962916\n",
      "4 0.0013453292726588928\n",
      "5 0.0013381109501742542\n",
      "6 0.0012614553011848957\n",
      "7 0.001106183056327947\n",
      "8 0.0009913863734484585\n",
      "9 0.0008757571024339127\n",
      "10 0.0007338984410219489\n",
      "11 0.0006864509257048199\n",
      "12 0.0005775119638534283\n",
      "13 0.00048018933575138346\n",
      "14 0.00037368116723659746\n",
      "15 0.0002783396432771148\n",
      "16 0.00021805448702410285\n",
      "17 0.00018112696097078769\n",
      "18 0.00014298112213567696\n",
      "19 0.000120601465357245\n",
      "20 0.00011512104828964503\n",
      "21 0.00010645510225767944\n",
      "22 9.898418290156401e-05\n",
      "23 9.471556040883543e-05\n",
      "24 8.981066362311253e-05\n",
      "25 8.515503292730198e-05\n",
      "26 8.086352397337016e-05\n",
      "27 7.612173743806715e-05\n",
      "28 7.182886799448558e-05\n",
      "29 6.86133687423989e-05\n"
     ]
    }
   ],
   "source": [
    "n=Perceptron(X_train,Labels)\n",
    "n.connect(X_train,Labels)\n",
    "n.train(batches=1000,lr=0.2,epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "OhDzZkLr-Bg0",
    "outputId": "58dea9f6-b86b-4369-c100-1f91f5007ddd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([311, 333, 303, 327, 330, 280, 286, 325, 234, 271]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z2jK6jKC-Il3",
    "outputId": "be985372-bf87-4242-d9c2-5d2e78094c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 89.0 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3: Extend your code from problem 2 to support a single layer neural network with N hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klZhm_WFDeV-"
   },
   "source": [
    "## Solution 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYE-TondDhAH"
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class SingleLayerNeuralNetwork():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255  \n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) \n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                \n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "\n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "id": "wffR59W-D9zR",
    "outputId": "8c31cd9c-7a48-4e1e-b839-4c37c4f88479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.00026507369571026614\n",
      "1 0.00025858462939753183\n",
      "2 0.0003219614067774773\n",
      "3 0.0003969718784473034\n",
      "4 0.00040219823644652093\n",
      "5 0.0003539709694831452\n",
      "6 0.0002872648113062959\n",
      "7 0.00022643033055996613\n",
      "8 0.00017733474096884785\n",
      "9 0.00013823525845028798\n",
      "10 0.00010853908154940441\n",
      "11 8.745823571803799e-05\n",
      "12 7.289272699810053e-05\n",
      "13 6.290169997826579e-05\n",
      "14 5.5982952283681096e-05\n",
      "15 5.0701449324372964e-05\n",
      "16 4.612029222306469e-05\n",
      "17 4.2153518585174634e-05\n",
      "18 3.894917880239001e-05\n",
      "19 3.640450110123102e-05\n",
      "20 3.4349600142749704e-05\n",
      "21 3.265893785975352e-05\n",
      "22 3.1218027519601566e-05\n",
      "23 2.9930836254492685e-05\n",
      "24 2.8743384629089865e-05\n",
      "25 2.7640925347847073e-05\n",
      "26 2.6626127615626717e-05\n",
      "27 2.5691004651994144e-05\n",
      "28 2.481661925753757e-05\n",
      "29 2.3986358964226792e-05\n",
      "30 2.3192789990938606e-05\n",
      "31 2.243617005877505e-05\n",
      "32 2.17189028243342e-05\n",
      "33 2.1041594716529147e-05\n",
      "34 2.0402456071458396e-05\n",
      "35 1.9798332478703507e-05\n",
      "36 1.9225746165598018e-05\n",
      "37 1.8681471640113977e-05\n",
      "38 1.8162740072012184e-05\n",
      "39 1.7667251520627377e-05\n",
      "40 1.7193113843307993e-05\n",
      "41 1.6738766155371304e-05\n",
      "42 1.6302909147056948e-05\n",
      "43 1.5884448022904003e-05\n",
      "44 1.5482447477912558e-05\n",
      "45 1.5096096376830381e-05\n",
      "46 1.472467983609257e-05\n",
      "47 1.4367556963838973e-05\n",
      "48 1.4024143085109682e-05\n",
      "49 1.3693895694613838e-05\n"
     ]
    }
   ],
   "source": [
    "n=SingleLayerNeuralNetwork(X_train,Labels)\n",
    "l1=Layer(100)\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "dp0shsvSEWvN",
    "outputId": "5039d921-26a6-43ba-e8a4-eba8489e2e7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([302, 322, 293, 330, 310, 264, 296, 327, 253, 303]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "knkPcfW8EYx5",
    "outputId": "c319fd28-79f5-4a04-9aec-152f3acd2ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 91.93333333333334 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4: Extend your code from problem 3 (use cross entropy error) and implement a 2-layer neural\n",
    "network, starting with a simple architecture containing N hidden units in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AcxS5MRXF1km"
   },
   "source": [
    "## Solution 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_nmBd8mF4d5"
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class DoubleLayerNeuralNetwork():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   \n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) \n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)    \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                \n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "xJY6j6AiGCI3",
    "outputId": "cde1989d-7336-40e9-b2ad-a6ec109102ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0006147752088982143\n",
      "1 0.00022125939912860426\n",
      "2 0.00011095514297885743\n",
      "3 7.204884572833662e-05\n",
      "4 5.308982882593679e-05\n",
      "5 4.081481599292317e-05\n",
      "6 3.72397198080972e-05\n",
      "7 3.457702926569818e-05\n",
      "8 3.6016675373810834e-05\n",
      "9 4.140543148333343e-05\n",
      "10 4.646969123463548e-05\n",
      "11 4.9277103508864534e-05\n",
      "12 5.152655618035941e-05\n",
      "13 5.163862117340579e-05\n",
      "14 4.846590370833118e-05\n",
      "15 4.41444585177594e-05\n",
      "16 3.972547920999699e-05\n",
      "17 3.530803170294093e-05\n",
      "18 3.119814553556046e-05\n",
      "19 2.776326929273412e-05\n"
     ]
    }
   ],
   "source": [
    "n=DoubleLayerNeuralNetwork(X_train,Labels)\n",
    "l1=Layer(100)\n",
    "l2=Layer(100)\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "mRAWf6C5GJgN",
    "outputId": "25d9b540-8076-4504-9130-dd1c5638ee66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([306, 335, 294, 320, 327, 288, 283, 332, 238, 277]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xmpGuKUbGMJ1",
    "outputId": "2ab4b7e5-9e2d-4eae-c400-cba966b2f011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 91.36666666666666 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5: Extend your code from problem 4 to implement different activations functions which will be\n",
    "passed as a parameter. In this problem all activations (except the final layer which should remain a\n",
    "softmax) must be changed to the passed activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCuNTnWEGOub"
   },
   "source": [
    "## Solution 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-KrbHw9GlsV"
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class NeuralNetworkActivations():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255  \n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        elif name=='relu':\n",
    "            if derivative==False:\n",
    "                return np.maximum(0.0,z)\n",
    "            else:\n",
    "              z[z<=0] = 0.0\n",
    "              z[z>0] = 1.0\n",
    "              return z\n",
    "        elif name=='tanh':\n",
    "          if derivative==False:\n",
    "                return np.tanh(z)\n",
    "          else:\n",
    "                return 1.0 - (np.tanh(z)) ** 2\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "JjlQBHZCHFWA",
    "outputId": "2be080cb-ee39-4697-b167-ed17e0f9e538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.393954821616651e-05\n",
      "1 0.0005118562710481734\n",
      "2 0.00023085754588267473\n",
      "3 0.00016826567329880513\n",
      "4 0.0002573099462506769\n",
      "5 0.0006350785332999381\n",
      "6 4.7923176876536626e-05\n",
      "7 3.840975191833874e-05\n",
      "8 3.430696135163668e-05\n",
      "9 1.0051623852615276e-05\n",
      "10 0.0005127406062526722\n",
      "11 4.717236913963525e-06\n",
      "12 0.00034750915286958333\n",
      "13 2.7764888942601524e-06\n",
      "14 7.91851544017604e-06\n",
      "15 5.149784307016578e-07\n",
      "16 1.7189875833741053e-05\n",
      "17 1.1657772127244548e-05\n",
      "18 2.1612809181756667e-05\n",
      "19 1.0198810562150977e-06\n"
     ]
    }
   ],
   "source": [
    "n=NeuralNetworkActivations(X_train,Labels)\n",
    "l1=Layer(100,'sigmoid')\n",
    "l2=Layer(50, 'tanh')\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "qZfek1XaL2PR",
    "outputId": "ff1d0cea-00ad-407d-eaa0-8e11f8c4b5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([305, 329, 296, 331, 308, 265, 286, 338, 246, 296]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sd2UmE1jNEUV",
    "outputId": "eb10fff3-62d2-4332-ca7b-0e9658d65120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 92.9 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 6: Extend your code from problem 5 to implement momentum with your gradient descent. The\n",
    "momentum value will be passed as a parameter. Your function should perform “epoch” number of\n",
    "epochs and return the resulting weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFVOwq1RQLVo"
   },
   "source": [
    "## Solution 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyyRnpPaQRSS"
   },
   "outputs": [],
   "source": [
    "    class Layer():\n",
    "        \"\"\"\n",
    "        size: Number of nodes in the hidden layer \n",
    "        activation: name of activation function for the layer\n",
    "        \"\"\"\n",
    "        def __init__(self,size,activation='sigmoid'): \n",
    "            self.shape=(1,size)\n",
    "            self.activation=activation\n",
    "\n",
    "    class NeuralNetworkMomentum():\n",
    "        def __init__(self,x,y):\n",
    "            \"\"\"\n",
    "            x is 2d array of input images\n",
    "            y are one hot encoded labels \n",
    "            \"\"\"\n",
    "            self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "            self.y=y\n",
    "            self.weights=[]\n",
    "            self.bias=[]\n",
    "            self.outputs=[]\n",
    "            self.derivatives=[]\n",
    "            self.activations=[]\n",
    "            self.delta_weights=[]\n",
    "            self.delta_bias=[]\n",
    "\n",
    "        def connect(self,layer1,layer2):\n",
    "            \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "            #Initialise weights,derivatives and activation lists\n",
    "            self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "            self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "            self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "            self.delta_weights.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
    "            self.delta_bias.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
    "            if isinstance(layer2,Layer):\n",
    "                self.activations.append(layer2.activation)\n",
    "\n",
    "        def activation(self,name,z,derivative=False):\n",
    "\n",
    "            #implementation of various activation functions and their derivatives\n",
    "            if name=='sigmoid':\n",
    "                if derivative==False:\n",
    "                    return 1/(1+np.exp(-z))\n",
    "                else:\n",
    "                    return z*(1-z)\n",
    "            elif name=='relu':\n",
    "                if derivative==False:\n",
    "                    return np.maximum(0.0,z)\n",
    "                else:\n",
    "                  z[z<=0] = 0.0\n",
    "                  z[z>0] = 1.0\n",
    "                  return z\n",
    "            elif name=='tanh':\n",
    "              if derivative==False:\n",
    "                    return np.tanh(z)\n",
    "              else:\n",
    "                    return 1.0 - (np.tanh(z)) ** 2\n",
    "\n",
    "        def softmax(self,z):\n",
    "            e=np.exp(z)\n",
    "            return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "\n",
    "        def max_log_likelihood(self,y_pred,y):\n",
    "            \"\"\"cross entropy\"\"\"\n",
    "            return y*np.log(y_pred)\n",
    "\n",
    "        def delta_mll(self,y,y_pred):\n",
    "            \"\"\"derivative of cross entropy\"\"\"\n",
    "            #return y*(y_pred-1)\n",
    "            return y_pred-y\n",
    "\n",
    "        def forward_pass(self,x,y,weights,bias):\n",
    "            cost=0\n",
    "            self.outputs=[]\n",
    "            for i in range(len(weights)):\n",
    "                samples=len(x)\n",
    "                ones_array=np.ones(samples).reshape(samples,1)\n",
    "                self.outputs.append(x) #append without adding ones array\n",
    "                z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "                if i==len(weights)-1:\n",
    "                    x=self.softmax(z)\n",
    "                else:\n",
    "                    x=self.activation(self.activations[i],z)\n",
    "            self.outputs.append(x)\n",
    "            self.y_pred=x\n",
    "\n",
    "            temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "            cost=np.mean(np.sum(temp,axis=1))\n",
    "            return cost\n",
    "\n",
    "\n",
    "        def backward_pass(self,y,lr,momentum=False,beta=0.5):\n",
    "            for i in range(len(self.weights)-1,-1,-1):\n",
    "                ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "                if i==len(self.weights)-1:\n",
    "                    prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                    # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                    self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "                else:\n",
    "                    prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                    self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "                if momentum:\n",
    "                    self.delta_weights[i]=beta*self.delta_weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                    self.delta_bias[i]=beta*self.delta_bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                    self.weights[i]=self.weights[i]+self.delta_weights[i]\n",
    "                    self.bias[i]=self.bias[i]+self.delta_bias[i]\n",
    "                else:\n",
    "                    self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                    self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "\n",
    "        def train(self,batches,lr=1e-3,epoch=10,beta=0.5):\n",
    "            \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "            for epochs in range(epoch):\n",
    "                samples=len(self.x)\n",
    "                c=0\n",
    "                for i in range(batches):\n",
    "                  x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "                  y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "\n",
    "                  c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "                  self.backward_pass(y_batch,lr,momentum=True,beta=0.5)\n",
    "                print(epochs,c/batches)\n",
    "\n",
    "        def predict(self,x):\n",
    "            \"\"\"input: x_test values\"\"\"\n",
    "            x=x/255\n",
    "            for i in range(len(self.weights)):\n",
    "                samples=len(x)\n",
    "                ones_array=np.ones(samples).reshape(samples,1)\n",
    "                z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "                if i==len(self.weights)-1:\n",
    "                    x=self.softmax(z)\n",
    "                else:\n",
    "                    x=self.activation(self.activations[i],z)\n",
    "            return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "5KieS6HyQuwx",
    "outputId": "72ae514a-6211-4948-db17-cd5092f16b4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0007368871858396068\n",
      "1 4.896638893996719e-05\n",
      "2 0.00014723111518745942\n",
      "3 9.195402384843673e-05\n",
      "4 8.46722106169578e-05\n",
      "5 0.0004254245940298797\n",
      "6 0.0004518025915654164\n",
      "7 0.0005144148604648345\n",
      "8 0.00034785497903570513\n",
      "9 6.636344183019607e-05\n",
      "10 0.00017596769111351266\n",
      "11 1.2285565898844749e-05\n",
      "12 9.561366468641029e-06\n",
      "13 3.946537442020382e-05\n",
      "14 3.768419621026274e-06\n",
      "15 1.4724958080051827e-06\n",
      "16 9.895523345277317e-07\n",
      "17 1.1006430491781112e-06\n",
      "18 1.0262939221277626e-06\n",
      "19 9.465183321892398e-07\n"
     ]
    }
   ],
   "source": [
    "n=NeuralNetworkMomentum(X_train,Labels)\n",
    "l1=Layer(100,'sigmoid')\n",
    "l2=Layer(50, 'tanh')\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=500,lr=0.1,epoch=20,beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "M6I2UDHGQy7c",
    "outputId": "32569bcd-dc09-4d76-8c74-adb0949961cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([304, 326, 293, 323, 296, 271, 295, 335, 259, 298]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "apHXOBwRQ60d",
    "outputId": "bbad07c7-96a6-4ad9-dda1-627f6cf30b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 92.6 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Assignment9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
